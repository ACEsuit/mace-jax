# best run: rude-adiana

flags.dtype = "float32"
flags.debug = False  # If True, enables flags jax_debug_nans and jax_debug_infs
flags.seed = 7
flags.profile = False  # If True, profiles the execution time and normalization of the model, requires pip install git+https://github.com/mariogeiger/profile-nn-jax.git


logs.directory = "results"

datasets.train_path = "data/rmd17_aspirin_train.xyz"  # in units of eV and Ã…
datasets.test_path = "data/rmd17_aspirin_test.xyz"
datasets.r_max = 5.0
datasets.valid_fraction = 0.208
datasets.n_node = 106
datasets.n_edge = 1620
datasets.n_graph = 6



model.radial_basis = @bessel_basis
bessel_basis.number = 8

# model.radial_envelope = @u_envelope  # 10. @ epoch 350
# u_envelope.p = 5
model.radial_envelope = @soft_envelope  # 9.5 @ epoch 350

model.symmetric_tensor_product_basis = True  # symmetric is slightly worse but slightly faster
model.off_diagonal = False
model.max_ell = 3
model.num_interactions = 2
model.hidden_irreps = "256x0e + 256x1o"
# model.interaction_irreps = "o3_full"  # "o3_restricted" seems to be better
model.epsilon = 0.4
model.correlation = 3  # 4 is better but 5x slower
model.gate = @silu
model.readout_mlp_irreps = "16x0e"
model.num_species = 11
model.path_normalization = "path"  # "element" or "path", it affects the weights when mixing the different paths
model.gradient_normalization = "path"  # "element" or "path", it affects the relative learning rate of the parameters
# path_normalization = "element"     =>  out = (path1 + path2) / sqrt(len(path1) + len(path2))
# path_normalization = "path"        =>  out = (path1 / sqrt(len(path1)) + path2 / sqrt(len(path2))) / sqrt(2)
# gradient_normalization = "element" =>  out = W @ path  with W initialized with stddev = 1 / sqrt(len(path))
# gradient_normalization = "path"    =>  out = W @ path / sqrt(len(path))  with W initialized with stddev = 1
# gradient_normalization="element" corresponds to the more traditional approach (e.g. in torch.nn.Linear or haiku.Linear)
# In e3nn pytorch, the two options were "path" by default.
model.atomic_energies = "average"
model.avg_num_neighbors = "average"
# model.atomic_energies = {
#     1: -13.587222780835477,
#     6: -1029.4889999855063,
#     7: -1484.9814568572233,
#     8: -2041.9816003861047
# }


loss.energy_weight = 1.0
loss.forces_weight = 10.0
loss.stress_weight = 0.0


optimizer.algorithm = @amsgrad
optimizer.lr = 0.01
optimizer.max_num_epochs = 100
# optimizer.weight_decay = 5e-7
# optimizer.scheduler = @piecewise_constant_schedule
# piecewise_constant_schedule.boundaries_and_scales = {
#     100: 0.1,  # divide the learning rate by 10 after 100 epochs
#     1000: 0.1,  # divide the learning rate by 10 after 1000 epochs
# }

train.eval_interval = 5
train.patience = 2048
train.ema_decay = 0.99
train.eval_train = False  # if True, evaluates the whole training set at each eval_interval
train.eval_test = False
train.log_errors = "PerAtomMAE"
