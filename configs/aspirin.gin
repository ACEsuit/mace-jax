flags.dtype = "float32"
flags.debug = False  # If True, enables flags jax_debug_nans and jax_debug_infs
flags.seed = 7
flags.profile = False  # If True, profiles the execution time and normalization of the model, requires pip install git+https://github.com/mariogeiger/profile-nn-jax.git


logs.directory = "results"

datasets.train_path = "data/rmd17_aspirin_train.xyz"  # in units of eV and Ã…
datasets.test_path = "data/rmd17_aspirin_test.xyz"
datasets.r_max = 5.0
datasets.valid_fraction = 0.208
datasets.batch_size = 5
datasets.valid_batch_size = 10



model.radial_basis = @bessel_basis
bessel_basis.number = 8

model.radial_envelope = @u_envelope
u_envelope.p = 5

model.max_ell = 3
model.num_interactions = 2
model.hidden_irreps = "256x0e + 256x1o"
model.epsilon = 0.03  # the coefficient applied right before the EquivariantProductBasisBlock, if not specified, use 1.0 / sqrt(avg_num_neighbors)
model.correlation = 3
model.gate = @silu
model.readout_mlp_irreps = "16x0e"
model.num_species = 11
model.path_normalization = "path"  # "element" or "path", it affects the weights when mixing the different paths
model.gradient_normalization = "path"  # "element" or "path", it affects the relative learning rate of the parameters
# path_normalization = "element"     =>  out = (path1 + path2) / sqrt(len(path1) + len(path2))
# path_normalization = "path"        =>  out = (path1 / sqrt(len(path1)) + path2 / sqrt(len(path2))) / sqrt(2)
# gradient_normalization = "element" =>  out = W @ path  with W initialized with stddev = 1 / sqrt(len(path))
# gradient_normalization = "path"    =>  out = W @ path / sqrt(len(path))  with W initialized with stddev = 1
# gradient_normalization="element" corresponds to the more traditional approach (e.g. in torch.nn.Linear or haiku.Linear)
# In e3nn pytorch, the two options were "path" by default.

# model.atomic_energies = {
#     1: -13.587222780835477,
#     6: -1029.4889999855063,
#     7: -1484.9814568572233,
#     8: -2041.9816003861047
# }


loss.energy_weight = 1.0
loss.forces_weight = 10.0

optimizer.lr = 0.01
optimizer.max_num_epochs = 2048
# optimizer.weight_decay = 1e-3
# optimizer.scheduler = @piecewise_constant_schedule
# piecewise_constant_schedule.boundaries_and_scales = {
#     100: 0.1,  # divide the learning rate by 10 after 100 epochs
#     1000: 0.1,  # divide the learning rate by 10 after 1000 epochs
# }

train.eval_interval = 2
train.patience = 2048
train.ema_decay = 0.99
train.eval_train = False  # if True, evaluates the whole training set at each eval_interval
train.log_errors = "PerAtomMAE"
