flags.dtype = "float32"
flags.debug = False  # If True, enables flags jax_debug_nans and jax_debug_infs
flags.seed = 7
flags.profile = False  # If True, profiles the execution time and normalization of the model, requires pip install git+https://github.com/mariogeiger/profile-nn-jax.git


logs.directory = "results"

datasets.train_path = "data/hdf5/train"
datasets.valid_path = "data/hdf5/val"
datasets.test_path = None
datasets.r_max = 5.0
datasets.n_edge = 2048



model.radial_basis = @bessel_basis
bessel_basis.number = 8

model.radial_envelope = @soft_envelope

model.num_bessel = 8
model.num_polynomial_cutoff = 6
model.max_ell = 3
model.num_interactions = 2
model.hidden_irreps = "32x0e"
model.MLP_irreps = "16x0e"
model.correlation = 3  # 4 is better but 5x slower
model.gate = @silu
model.num_species = 11
model.atomic_energies = "average"
model.avg_num_neighbors = "average"
# model.atomic_energies = {
#     1: -13.587222780835477,
#     6: -1029.4889999855063,
#     7: -1484.9814568572233,
#     8: -2041.9816003861047
# }


loss.energy_weight = 1.0
loss.forces_weight = 10.0
loss.stress_weight = 0.0


optimizer.algorithm = @sgd
optimizer.lr = 0.01
optimizer.max_epochs = 2
# optimizer.weight_decay = 5e-7
# optimizer.scheduler = @piecewise_constant_schedule
# piecewise_constant_schedule.boundaries_and_scales = {
#     100: 0.1,  # divide the learning rate by 10 after 100 intervals
#     1000: 0.1,  # divide the learning rate by 10 after 1000 intervals
# }

train.patience = 2048
train.ema_decay = 0.99
train.eval_train = True  # if True, evaluates the whole training set at each eval_interval
train.eval_test = False
train.log_errors = "PerAtomMAE"
