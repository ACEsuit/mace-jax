"""Cue-equivariant symmetric contraction implemented with Flax.

This module bridges the Torch/cuEquivariance symmetric contraction operator used
throughout MACE with a Flax/JAX implementation.  It exposes a drop-in compatible
``SymmetricContraction`` layer that:

* Reconstructs the exact equivariant polynomial descriptor used by the
  reference Torch implementation.
* Handles both reduced and full Clebsch–Gordan bases.
* Allows the caller to pick a specific element embedding at apply time via an
  ``indices`` argument, mirroring the original torch-scatter based interfaces.
* Provides import utilities so state dictionaries from Torch can be moved to the
  JAX module without numerical drift.

The implementations in this file are intentionally verbose: each helper carries
docstrings describing the slightly idiosyncratic layout conversions required by
cuEquivariance and e3nn, and the import code documents the exact ordering of
weights that is expected during the conversion.
"""

from __future__ import annotations

from functools import cache

import cuequivariance_jax as cuex
import jax
import jax.numpy as jnp
import numpy as np
from e3nn_jax import Irreps
from flax import nnx

import cuequivariance as cue
from cuequivariance.group_theory.experimental.mace.symmetric_contractions import (
    symmetric_contraction as cue_mace_symmetric_contraction,
)
from mace_jax.adapters.nnx.torch import (
    _resolve_scope,
    nxx_auto_import_from_torch,
    nxx_register_import_mapper,
)
from mace_jax.nnx_config import ConfigVar
from mace_jax.nnx_utils import state_to_pure_dict
from mace_jax.tools.dtype import default_dtype

from .utility import ir_mul_to_mul_ir


@nxx_auto_import_from_torch(allow_missing_mapper=True)
class SymmetricContraction(nnx.Module):
    """Symmetric contraction layer evaluated with cuequivariance-jax.

    Parameters
    ----------
    irreps_in:
        e3nn irreps describing the incoming node features.  All entries must
        share a common multiplicity.
    irreps_out:
        Target irreps produced by the contraction.  The multiplicity must match
        that of ``irreps_in``.
    correlation:
        Maximum correlation order (degree) used by the symmetric contraction.
        The operator internally builds all degrees in ``[1, correlation]``.
    num_elements:
        Number of distinct chemical elements represented in the embedding table.
        The ``indices`` argument selects one of these entries per node.
    use_reduced_cg:
        When ``True`` (default) the operator stores weights in the reduced
        Clebsch–Gordan basis generated by cuEquivariance.  Setting ``False``
        expands the basis to the original dense CG space.
    input_layout:
        Expected ordering of the feature tensor passed to :meth:`__call__`.  The
        default ``'mul_ir'`` matches e3nn; ``'ir_mul'`` accepts the cuEquivariance
        layout instead.

    Notes
    -----
    The class stores the cuEquivariance descriptor built from
    ``experimental.mace.symmetric_contractions`` so that the exact polynomial
    used in Torch is evaluated in JAX.  All layout conversions are handled inside
    the module; callers only need to observe the ``input_layout`` argument.
    """

    irreps_in: Irreps
    irreps_out: Irreps
    correlation: int
    num_elements: int
    use_reduced_cg: bool = True
    input_layout: str = 'mul_ir'
    output_layout: str = 'mul_ir'
    group: object = cue.O3

    def __init__(
        self,
        irreps_in: Irreps,
        irreps_out: Irreps,
        correlation: int,
        num_elements: int,
        use_reduced_cg: bool = True,
        input_layout: str = 'mul_ir',
        output_layout: str = 'mul_ir',
        group: object = cue.O3,
        *,
        rngs: nnx.Rngs | None = None,
    ) -> None:
        """Validate configuration and construct the cuEquivariance descriptor.

        The routine performs a number of sanity checks that mirror the Torch
        implementation, computes the projection matrix when full CG weights are
        requested, and creates the cached descriptor that is reused on every
        forward pass.
        """
        self.irreps_in = irreps_in
        self.irreps_out = irreps_out
        self.correlation = correlation
        self.num_elements = num_elements
        self.use_reduced_cg = use_reduced_cg
        self.input_layout = input_layout
        self.output_layout = output_layout
        self.group = group

        if self.correlation <= 0:
            raise ValueError('correlation must be a positive integer')
        if self.num_elements <= 0:
            raise ValueError('num_elements must be positive')
        if self.input_layout not in {'mul_ir', 'ir_mul'}:
            raise ValueError(
                "input_layout must be either 'mul_ir' or 'ir_mul'; "
                f'got {self.input_layout!r}'
            )
        if self.output_layout not in {'mul_ir', 'ir_mul'}:
            raise ValueError(
                "output_layout must be either 'mul_ir' or 'ir_mul'; "
                f'got {self.output_layout!r}'
            )

        input_layout_code = 0 if self.input_layout == 'mul_ir' else 1
        output_layout_code = 0 if self.output_layout == 'mul_ir' else 1
        self.input_layout_config = ConfigVar(
            jnp.asarray(input_layout_code, dtype=jnp.int32),
            is_mutable=False,
        )
        self.output_layout_config = ConfigVar(
            jnp.asarray(output_layout_code, dtype=jnp.int32),
            is_mutable=False,
        )

        irreps_in_o3 = Irreps(self.irreps_in)
        irreps_out_o3 = Irreps(self.irreps_out)
        self.irreps_in_o3_str = str(irreps_in_o3)
        self.irreps_out_o3_str = str(irreps_out_o3)

        muls_in = {mul for mul, _ in irreps_in_o3}
        muls_out = {mul for mul, _ in irreps_out_o3}
        if len(muls_in) != 1 or len(muls_out) != 1 or muls_in != muls_out:
            raise ValueError(
                'SymmetricContraction requires all input/output irreps to share the same multiplicity'
            )
        self.mul = next(iter(muls_in))

        self.irreps_in_cue = cue.Irreps(self.group, irreps_in_o3)
        self.irreps_out_cue = cue.Irreps(self.group, irreps_out_o3)
        self.feature_dim = sum(ir.dim for _, ir in irreps_in_o3)
        self.irreps_in_cue_base = self.irreps_in_cue.set_mul(1)
        self.irreps_out_dim = irreps_out_o3.dim

        degrees = tuple(range(1, self.correlation + 1))
        descriptor, projection = cue_mace_symmetric_contraction(
            self.irreps_in_cue,
            self.irreps_out_cue,
            degrees,
        )
        self.descriptor = descriptor
        self.weight_irreps = descriptor.inputs[0].irreps
        self.weight_numel = self.weight_irreps.dim

        if self.use_reduced_cg:
            self.projection = None
            if projection is not None:
                self.weight_basis_dim = int(np.asarray(projection).shape[1])
            else:
                self.weight_basis_dim = self.weight_numel // self.mul
        else:
            self.projection = jnp.asarray(projection)
            self.weight_basis_dim = self.projection.shape[0]

        self.weight_param_shape = (self.num_elements, self.weight_basis_dim, self.mul)
        if rngs is None:
            raise ValueError('rngs is required to initialize SymmetricContraction')
        self.weight = nnx.Param(
            jax.random.normal(
                rngs(),
                self.weight_param_shape,
                dtype=default_dtype(),
            )
        )

    def _weight_param(self) -> jnp.ndarray:
        """Return the learnable basis weights initialised with Gaussian noise.

        Returns
        -------
        jnp.ndarray
            Parameter tensor of shape ``(num_elements, basis_dim, mul)`` stored
            in the current Flax variable collection.
        """
        return self.weight

    def _ensure_mul_ir_layout(self, x: jnp.ndarray) -> jnp.ndarray:
        """Convert inputs to mul_ir layout if needed.

        Parameters
        ----------
        x:
            Feature tensor with either ``mul_ir`` or ``ir_mul`` ordering.

        Returns
        -------
        jnp.ndarray
            Tensor guaranteed to be in ``mul_ir`` layout.
        """
        if self.input_layout == 'ir_mul':
            return self._convert_ir_mul_to_mul_ir(x)
        return x

    def _features_to_rep_ir_mul(
        self, x: jnp.ndarray, dtype: jnp.dtype
    ) -> cuex.RepArray:
        """Pack ir_mul features into cue RepArray segments without layout conversion."""
        segments: list[jnp.ndarray] = []
        offset = 0
        for mul_ir in self.irreps_in_cue_base:
            width = mul_ir.ir.dim
            seg = x[:, offset : offset + width, :]
            if seg.shape[1] != width:
                raise ValueError('Input feature dimension mismatch with irreps.')
            segments.append(seg)
            offset += width

        if offset != x.shape[1]:
            raise ValueError('Input feature dimension mismatch with irreps.')

        return cuex.from_segments(
            self.irreps_in_cue,
            segments,
            (x.shape[0], self.mul),
            cue.ir_mul,
            dtype=dtype,
        )

    def _project_basis_weights(
        self, basis_weights: jnp.ndarray, dtype: jnp.dtype
    ) -> jnp.ndarray:
        """Project basis weights to the full Clebsch–Gordan space.

        Parameters
        ----------
        basis_weights:
            Learned weights in the reduced space.
        dtype:
            Desired data type of the projected weights.

        Returns
        -------
        jnp.ndarray
            Weights expressed in the basis expected by the descriptor.
        """
        if self.projection is None:
            return basis_weights.astype(dtype)
        projection = jnp.asarray(self.projection, dtype=dtype)
        return jnp.einsum('zau,ab->zbu', basis_weights.astype(dtype), projection)

    def _weight_rep_from_indices(
        self,
        indices: jnp.ndarray,
        dtype: jnp.dtype,
    ) -> cuex.RepArray:
        """Select element weights and wrap them as a cue RepArray.

        Parameters
        ----------
        indices:
            Int tensor of shape ``(batch,)`` or mixing matrix of shape
            ``(batch, num_elements)`` selecting the element embedding.
        dtype:
            Desired numeric type.

        Returns
        -------
        cuex.RepArray
            Representation-aware view of the selected weights.
        """
        basis_weights = self._weight_param()
        projected = self._project_basis_weights(basis_weights, dtype)
        weight_flat = projected.reshape(self.num_elements, self.weight_numel)
        selected = _select_weights(
            weight_flat,
            indices,
            dtype=dtype,
            num_elements=self.num_elements,
        )
        return cuex.RepArray(self.weight_irreps, selected, cue.ir_mul)

    def __call__(
        self,
        x: jnp.ndarray,
        indices: jnp.ndarray,
    ) -> jnp.ndarray:
        """Apply the symmetric contraction and return mul_ir-ordered features.

        Parameters
        ----------
        x:
            Either a flattened ``(batch, mul * feature_dim)`` array or the
            explicit ``(batch, mul, feature_dim)`` view containing the node
            features ready for contraction.
        indices:
            Element identifiers or mixing matrices selecting the weight slice.

        Returns
        -------
        jnp.ndarray
            Contracted features in ``mul_ir`` layout matching ``irreps_out``.
        """
        array = jnp.asarray(x)
        dtype = array.dtype

        if array.ndim == 2:
            expected = self.mul * self.feature_dim
            if array.shape[1] != expected:
                raise ValueError(
                    'Flattened features must have size '
                    f'{expected}; received {array.shape[1]}'
                )
            if self.input_layout == 'mul_ir':
                array = array.reshape(array.shape[0], self.mul, self.feature_dim)
            elif self.input_layout == 'ir_mul':
                array = array.reshape(array.shape[0], self.feature_dim, self.mul)
            else:
                raise ValueError(f'Unsupported input_layout {self.input_layout!r}')
        elif array.ndim != 3:
            raise ValueError(
                'SymmetricContraction expects inputs with rank 2 or 3; '
                f'got rank {array.ndim}'
            )

        weight_rep = self._weight_rep_from_indices(indices, dtype)
        if self.input_layout == 'ir_mul':
            _validate_features_ir_mul(array, self.mul, self.feature_dim)
            x_rep = self._features_to_rep_ir_mul(array, dtype)
        else:
            array = self._ensure_mul_ir_layout(array)
            _validate_features(array, self.mul, self.feature_dim)
            x_rep = self._features_to_rep(array, dtype)

        out_rep = cuex.equivariant_polynomial(
            self.descriptor,
            [weight_rep, x_rep],
            math_dtype=dtype,
            method='naive',
        )

        out_ir_mul = out_rep.change_layout(cue.ir_mul).array

        if self.output_layout == 'ir_mul':
            return out_ir_mul
        return ir_mul_to_mul_ir(
            out_ir_mul,
            Irreps(self.irreps_out_o3_str),
        )

    def _features_to_rep(self, x: jnp.ndarray, dtype: jnp.dtype) -> cuex.RepArray:
        """Pack mul_ir features into cue RepArray segments.

        Parameters
        ----------
        x:
            Feature tensor in mul_ir layout of shape ``(batch, mul, feature_dim)``.
        dtype:
            Desired dtype of the resulting representation array.

        Returns
        -------
        cuex.RepArray
            Features understood by cuEquivariance, segmented per irrep.
        """
        segments: list[jnp.ndarray] = []
        offset = 0
        for mul_ir in self.irreps_in_cue_base:
            width = mul_ir.ir.dim
            seg = x[:, :, offset : offset + width]
            if seg.shape[-1] != width:
                raise ValueError('Input feature dimension mismatch with irreps.')
            segments.append(jnp.swapaxes(seg, -2, -1))
            offset += width

        return cuex.from_segments(
            self.irreps_in_cue,
            segments,
            (x.shape[0], self.mul),
            cue.ir_mul,
            dtype=dtype,
        )

    def _convert_ir_mul_to_mul_ir(self, x: jnp.ndarray) -> jnp.ndarray:
        """Reorder ir_mul input layout to mul_ir."""
        segments: list[jnp.ndarray] = []
        offset = 0
        for mul_ir in self.irreps_in_cue_base:
            width = mul_ir.ir.dim
            seg = x[:, offset : offset + width, :]
            if seg.shape[1] != width:
                raise ValueError('Input feature dimension mismatch with irreps.')
            segments.append(jnp.swapaxes(seg, -1, -2))
            offset += width

        if offset != x.shape[1]:
            raise ValueError('Input feature dimension mismatch with irreps.')

        if not segments:
            return jnp.swapaxes(x, -1, -2)

        return jnp.concatenate(segments, axis=-1)


## Utilities
## ----------------------------------------------------------------------------


def _validate_features(x: jnp.ndarray, mul: int, feature_dim: int) -> None:
    """Ensure the feature tensor shape matches the expected mul_ir layout.

    Raises
    ------
    ValueError
        If ``x`` does not conform to ``(batch, mul, feature_dim)``.
    """
    if x.ndim != 3 or x.shape[1] != mul or x.shape[2] != feature_dim:
        raise ValueError(
            'SymmetricContraction expects input with shape '
            f'(batch, {mul}, {feature_dim}); got {tuple(x.shape)}'
        )


def _validate_features_ir_mul(x: jnp.ndarray, mul: int, feature_dim: int) -> None:
    """Ensure the feature tensor shape matches the expected ir_mul layout."""
    if x.ndim != 3 or x.shape[1] != feature_dim or x.shape[2] != mul:
        raise ValueError(
            'SymmetricContraction expects input with shape '
            f'(batch, {feature_dim}, {mul}); got {tuple(x.shape)}'
        )


def _select_weights(
    weight_flat: jnp.ndarray,
    selector: jnp.ndarray,
    *,
    dtype: jnp.dtype,
    num_elements: int,
) -> jnp.ndarray:
    """Select element weights by index or mixing matrix.

    Parameters
    ----------
    weight_flat:
        Weight tensor flattened to ``(num_elements, weight_dim)``.
    selector:
        Either integer indices of shape ``(batch,)`` or a mixing matrix
        ``(batch, num_elements)``.
    dtype:
        Target dtype for the selection.
    num_elements:
        Upper bound used to validate integer indices.

    Returns
    -------
    jnp.ndarray
        Per-example weights ready to be wrapped in a :class:`RepArray`.
    """
    selector = jnp.asarray(selector)
    if selector.ndim == 1:
        idx = selector.astype(jnp.int32)
        invalid = jnp.any(idx < 0) | jnp.any(idx >= num_elements)

        def _raise(_):
            jax.debug.callback(_raise_invalid_indices, jnp.int32(0))
            return jnp.int32(0)

        jax.lax.cond(invalid, _raise, lambda _: jnp.int32(0), operand=jnp.int32(0))
        return weight_flat[idx]

    if selector.ndim == 2:
        if selector.shape[1] != num_elements:
            raise ValueError('Mixing matrix must have second dimension num_elements')
        mix = jnp.asarray(selector, dtype=dtype)
        return mix @ weight_flat

    raise ValueError('indices must be rank-1 (element ids) or rank-2 (mixing matrix)')


def _raise_invalid_indices(_: jnp.ndarray) -> None:
    raise ValueError('indices out of range for the available elements')


def _symmetric_contraction_import_from_torch_with_layout(cls, torch_module, variables):
    """Wrapper around the auto-generated import that enforces layout parity."""
    expected_input = variables.get('input_layout_config', None)
    expected_output = variables.get('output_layout_config', None)

    def _decode_layout(val):
        if isinstance(val, jnp.ndarray):
            try:
                val_int = int(val)
            except Exception:
                return None
            return 'mul_ir' if val_int == 0 else 'ir_mul'
        if isinstance(val, (int, np.integer)):
            return 'mul_ir' if int(val) == 0 else 'ir_mul'
        return val

    def _layout_str_from_obj(layout_obj) -> str | None:
        if layout_obj is None:
            return None
        if isinstance(layout_obj, str):
            return layout_obj
        for attr in ('layout_str', 'name', '__name__'):
            val = getattr(layout_obj, attr, None)
            if val is not None:
                return str(val)
        return str(layout_obj)

    expected_input = _decode_layout(expected_input)
    expected_output = _decode_layout(expected_output)

    torch_input = _layout_str_from_obj(getattr(torch_module, 'input_layout', None))
    torch_output = _layout_str_from_obj(getattr(torch_module, 'output_layout', None))
    if torch_input is None or torch_output is None:
        descriptor = getattr(torch_module, 'descriptor', None) or getattr(
            torch_module, '_descriptor', None
        )
        if descriptor is not None:
            if torch_input is None:
                try:
                    torch_input = _layout_str_from_obj(descriptor.inputs[1].layout)
                except Exception:
                    torch_input = None
            if torch_output is None:
                try:
                    torch_output = _layout_str_from_obj(descriptor.outputs[0].layout)
                except Exception:
                    torch_output = None

    if torch_input is None:
        torch_input = 'mul_ir'
    if torch_output is None:
        torch_output = 'mul_ir'

    if expected_input is not None and str(expected_input) != str(torch_input):
        raise ValueError(
            f'JAX SymmetricContraction expected input layout {expected_input!r} but '
            f'Torch module uses layout {torch_input!r}.'
        )
    if expected_output is not None and str(expected_output) != str(torch_output):
        raise ValueError(
            f'JAX SymmetricContraction expected output layout {expected_output!r} but '
            f'Torch module uses layout {torch_output!r}.'
        )

    return cls._import_from_torch_impl(
        torch_module,
        variables,
        skip_root=False,
    )


# Override the auto-generated import_from_torch with layout validation.
SymmetricContraction.import_from_torch = classmethod(
    _symmetric_contraction_import_from_torch_with_layout
)


## Import functions
## ----------------------------------------------------------------------------


@nxx_register_import_mapper(
    'cuequivariance_torch.operations.symmetric_contraction.SymmetricContraction'
)
def _import_cue_symmetric_contraction(module, variables, scope) -> None:
    """Import mapper that transfers weights from the cuEquivariance Torch module.

    The Torch implementation stores its parameter as a dense tensor in the
    layout expected by cueEquivariance, so we only need to copy it over while
    preserving dtype.  The helper is registered so ``init_from_torch`` can reuse
    the logic automatically.
    """
    target = _resolve_scope(variables, scope)
    weight = module.weight.detach().cpu().numpy()
    existing = target.get('weight')
    dtype = existing.dtype if existing is not None else weight.dtype
    target['weight'] = jnp.asarray(weight, dtype=dtype)


@nxx_register_import_mapper('mace.modules.symmetric_contraction.SymmetricContraction')
def _import_native_symmetric_contraction(module, variables, scope) -> None:
    """Import mapper that mirrors the native Torch module weight layout.

    Native MACE stores weights per-degree and per-correlation.  This mapper
    reorders and (optionally) projects them into the reduced basis accepted by
    the JAX operator.
    """
    target = _resolve_scope(variables, scope)
    weight = target.get('weight')
    converted = _convert_native_weights(
        module,
        target_template=jnp.asarray(weight, dtype=default_dtype()),
    )
    target['weight'] = jnp.asarray(converted, dtype=weight.dtype)


## Import helpers
## ----------------------------------------------------------------------------


def _convert_native_weights(
    torch_module,
    *,
    target_template: jnp.ndarray,
) -> jnp.ndarray:
    """Convert native MACE weights into the layout expected by the JAX module.

    Parameters
    ----------
    torch_module:
        Instance of the native PyTorch symmetric contraction module whose
        parameters are being imported.
    target_template:
        Array matching the desired output shape and dtype.  Only its metadata is
        used; the values are ignored.

    Returns
    -------
    jnp.ndarray
        Weight tensor expressed in the basis required by the JAX module.

    Notes
    -----
    If the torch module uses the reduced CG basis we simply apply the square
    projection that aligns the native ordering with the cue descriptor.  For
    full CG weights we first compute a cached linear transform that remaps the
    native ordering to the canonical cue ordering before optionally reducing the
    tensor to the projected basis.
    """
    from mace.tools.cg_cueq_tools import (  # noqa: PLC0415
        symmetric_contraction_proj as mace_symmetric_contraction_proj,
    )

    irreps_in = Irreps(str(torch_module.irreps_in))
    irreps_out = Irreps(str(torch_module.irreps_out))
    correlation = int(torch_module.contractions[0].correlation)
    num_elements = int(torch_module.contractions[0].weights_max.shape[0])

    mul = irreps_in[0].mul
    feature_dim = sum(term.ir.dim for term in irreps_in)
    if mul == 0 or feature_dim == 0:
        return target_template

    weight_shape = target_template.shape
    _, basis_dim, mul_dim = weight_shape
    if basis_dim == 0 or mul_dim == 0:
        return target_template

    native_weight = _gather_native_reduced_weights(
        torch_module,
        correlation=correlation,
        mul_dim=mul_dim,
        num_elements=num_elements,
    )

    cue_irreps_in = cue.Irreps(cue.O3, str(irreps_in))
    cue_irreps_out = cue.Irreps(cue.O3, str(irreps_out))
    degrees = tuple(range(1, correlation + 1))
    _, reduced_projection = mace_symmetric_contraction_proj(
        cue_irreps_in,
        cue_irreps_out,
        degrees,
    )
    reduced_projection = np.asarray(reduced_projection, dtype=native_weight.dtype)

    _, descriptor_projection = cue_mace_symmetric_contraction(
        cue_irreps_in,
        cue_irreps_out,
        degrees,
    )
    descriptor_projection = np.asarray(descriptor_projection, dtype=native_weight.dtype)

    reduced_dim = reduced_projection.shape[1]
    full_dim = descriptor_projection.shape[0]
    native_dim = native_weight.shape[1]

    if basis_dim == reduced_dim:
        if native_dim == reduced_dim:
            converted = np.einsum(
                'zau,ab->zbu', native_weight, reduced_projection, optimize=True
            )
        elif native_dim == full_dim:
            transform = _compute_full_cg_transform(irreps_in, irreps_out, correlation)
            transform = np.asarray(transform, dtype=native_weight.dtype)
            canonical = np.einsum(
                'ab,zbu->zau', transform, native_weight, optimize=True
            )
            converted = np.einsum(
                'zau,ab->zbu', canonical, descriptor_projection, optimize=True
            )
        else:
            raise ValueError(
                'Native SymmetricContraction weight shape mismatch during import.'
            )
    elif basis_dim == full_dim:
        if native_dim == full_dim:
            transform = _compute_full_cg_transform(irreps_in, irreps_out, correlation)
            transform = np.asarray(transform, dtype=native_weight.dtype)
            converted = np.einsum(
                'ab,zbu->zau', transform, native_weight, optimize=True
            )
        elif native_dim == reduced_dim:
            lift = np.linalg.pinv(descriptor_projection, rcond=1e-12).astype(
                native_weight.dtype, copy=False
            )
            converted = np.einsum('zau,ab->zbu', native_weight, lift, optimize=True)
        else:
            raise ValueError(
                'Native SymmetricContraction weight shape mismatch during import.'
            )
    else:
        raise ValueError(
            'Native SymmetricContraction weight shape mismatch during import.'
        )
    return jnp.asarray(converted, dtype=target_template.dtype)


def _gather_native_reduced_weights(
    torch_module,
    *,
    correlation: int,
    mul_dim: int,
    num_elements: int,
) -> np.ndarray:
    """Stack native-use weights in the order expected by the cue projection.

    The helper extracts the hierarchical weight tensors from the Torch module
    (``weights_max`` plus optional ``weights`` lists), zeroes out disabled
    contractions, and concatenates them degree-by-degree so that the projection
    matrix can be applied directly.
    """

    base_array = torch_module.contractions[0].weights_max.detach().cpu().numpy()
    dtype = np.asarray(base_array).dtype

    if correlation <= 0:
        return np.zeros((num_elements, 0, mul_dim), dtype=dtype)

    native_blocks: list[np.ndarray] = []
    for contraction in torch_module.contractions:
        degree_blocks: list[np.ndarray] = []

        for degree in range(correlation, 0, -1):
            if degree == correlation:
                weight_param = contraction.weights_max
                zeroed = bool(getattr(contraction, 'weights_max_zeroed', False))
            else:
                idx = correlation - degree - 1
                weight_param = contraction.weights[idx]
                zeroed = bool(getattr(contraction, f'weights_{idx}_zeroed', False))

            array = np.asarray(weight_param.detach().cpu().numpy(), dtype=dtype)
            if zeroed:
                array = np.zeros_like(array)

            if array.shape[1] == 0:
                continue
            degree_blocks.append(array)

        if degree_blocks:
            native_blocks.append(np.concatenate(degree_blocks, axis=1))

    if native_blocks:
        stacked = np.concatenate(native_blocks, axis=1)
    else:
        stacked = np.zeros((num_elements, 0, mul_dim), dtype=dtype)

    if stacked.shape[0] != num_elements or stacked.shape[2] != mul_dim:
        raise ValueError('Native SymmetricContraction weights shape mismatch.')

    return stacked


def _compute_full_cg_transform(
    irreps_in: Irreps,
    irreps_out: Irreps,
    correlation: int,
) -> np.ndarray:
    """Return the native → canonical transform for the requested full CG setup.

    This is a convenience wrapper over the cached implementation that accepts
    :mod:`e3nn` irreps objects instead of their string representation.
    """
    base_in = Irreps(str(irreps_in)).set_mul(1)
    base_out = Irreps(str(irreps_out)).set_mul(1)
    return _cached_full_cg_transform(str(base_in), str(base_out), int(correlation))


@cache
def _cached_full_cg_transform(
    irreps_in_str: str,
    irreps_out_str: str,
    correlation: int,
) -> np.ndarray:
    """Compute the native → canonical transform for full CG weights.

    This function is decorated with :func:`functools.cache`, so the expensive
    linear-algebra work is only performed the first time a specific combination
    of irreps and correlation order is requested.
    """
    try:
        from e3nn import o3  # noqa: PLC0415
    except ModuleNotFoundError as exc:  # pragma: no cover - import guard
        raise ModuleNotFoundError(
            'Full CG transforms require the optional dependency "e3nn". '
            'Install mace with its PyTorch extras to enable this feature.'
        ) from exc

    try:
        from mace.modules.wrapper_ops import (  # noqa: PLC0415
            SymmetricContractionWrapper as TorchSymmetricContraction,
        )
    except ModuleNotFoundError as exc:  # pragma: no cover - import guard
        raise ModuleNotFoundError(
            'Full CG transforms rely on the PyTorch wrapper ops, which are only '
            'available when mace is installed with its torch components.'
        ) from exc

    irreps_in_o3 = o3.Irreps(irreps_in_str)
    irreps_out_o3 = o3.Irreps(irreps_out_str)

    torch_module = (
        TorchSymmetricContraction(
            irreps_in=irreps_in_o3,
            irreps_out=irreps_out_o3,
            correlation=correlation,
            num_elements=1,
            use_reduced_cg=False,
        )
        .float()
        .eval()
    )

    jax_module = SymmetricContraction(
        irreps_in=Irreps(irreps_in_str),
        irreps_out=Irreps(irreps_out_str),
        correlation=correlation,
        num_elements=1,
        use_reduced_cg=False,
        rngs=nnx.Rngs(0),
    )

    mul = o3.Irreps(irreps_in_str)[0].mul
    feature_dim = sum(term.ir.dim for term in Irreps(irreps_in_str))

    native_dim = _gather_native_reduced_weights(
        torch_module,
        correlation=correlation,
        mul_dim=mul,
        num_elements=1,
    ).shape[1]

    graphdef, state = nnx.split(jax_module)
    params = state_to_pure_dict(state)
    params_zero = _with_zero_weights(params)
    canonical_dim = int(np.asarray(params_zero['weight']).shape[1])

    batch = max(canonical_dim, native_dim)
    rng = np.random.default_rng(0)
    inputs_np = rng.standard_normal((batch, mul, feature_dim)).astype(np.float32)

    inputs_jax = jnp.asarray(inputs_np)
    indices_jax = jnp.zeros((batch,), dtype=jnp.int32)

    canonical_matrix = _canonical_design_matrix(
        graphdef,
        params_zero,
        inputs_jax,
        indices_jax,
    )
    native_matrix = _native_design_matrix(
        torch_module,
        correlation=correlation,
        basis_dim=native_dim,
        inputs_np=inputs_np,
    )

    transform = np.linalg.lstsq(canonical_matrix, native_matrix, rcond=1e-12)[0]
    return transform.astype(np.float64)


def _with_zero_weights(params: dict) -> dict:
    """Return a params pytree with all learnable weights zeroed."""
    params_mutable = dict(params)
    params_mutable['weight'] = jnp.zeros_like(params_mutable['weight'])
    return params_mutable


def _canonical_design_matrix(
    graphdef: nnx.GraphDef,
    params_zero: dict,
    inputs: jnp.ndarray,
    indices: jnp.ndarray,
) -> np.ndarray:
    """Evaluate the JAX module on the canonical cue basis vectors.

    The returned matrix contains one column per canonical basis element and is
    therefore the linear map from canonical weights to model outputs.  This is
    later paired with the native design matrix to recover the reordering
    transform via a least-squares solve.

    Returns
    -------
    np.ndarray
        Array with shape ``(output_dim, canonical_dim)`` capturing the response
        of the module to each canonical basis excitation.
    """
    weight_shape = np.asarray(params_zero['weight']).shape
    canonical_dim = weight_shape[1]

    outputs: list[np.ndarray] = []
    for idx in range(canonical_dim):
        weight = np.zeros(weight_shape, dtype=np.float32)
        weight[0, idx, 0] = 1.0

        params_idx = dict(params_zero)
        params_idx['weight'] = jnp.asarray(weight)

        out, _ = graphdef.apply(params_idx)(inputs, indices)
        outputs.append(np.asarray(out).reshape(-1))

    return np.stack(outputs, axis=1)


def _native_design_matrix(
    torch_module,
    *,
    correlation: int,
    basis_dim: int,
    inputs_np: np.ndarray,
) -> np.ndarray:
    """Evaluate the native module on its basis vectors using a shared batch.

    Each column of the returned matrix corresponds to the native module output
    when a single flattened basis vector is activated.  Pairing this matrix
    with :func:`_canonical_design_matrix` allows us to infer the change of
    basis between the two representations.

    Returns
    -------
    np.ndarray
        Array with shape ``(output_dim, basis_dim)`` whose columns capture the
        native module response for each basis vector.
    """
    import torch  # noqa: PLC0415

    torch_inputs = torch.tensor(inputs_np, dtype=torch.float32)
    num_elements = torch_module.contractions[0].weights_max.shape[0]
    torch_attrs = torch.ones((inputs_np.shape[0], num_elements), dtype=torch.float32)

    outputs: list[np.ndarray] = []
    for idx in range(basis_dim):
        basis = np.zeros(basis_dim, dtype=np.float32)
        basis[idx] = 1.0
        _assign_native_basis(torch_module, basis_vector=basis, correlation=correlation)
        with torch.no_grad():
            out = torch_module(torch_inputs, torch_attrs).cpu().numpy().reshape(-1)
        outputs.append(out)

    return np.stack(outputs, axis=1)


def _assign_native_basis(
    torch_module,
    *,
    basis_vector: np.ndarray,
    correlation: int,
) -> None:
    """Populate ``torch_module`` with the provided flattened native basis vector.

    The helper walks through the per-degree tensors stored inside the torch
    module, slicing the flattened vector into the appropriate blocks and
    writing them back into the ``weights_max``/``weights`` attributes.  A
    :class:`ValueError` is raised if the basis vector length does not match the
    expected native parameter size.
    """
    offset = 0
    import torch  # noqa: PLC0415

    with torch.no_grad():
        for contraction in torch_module.contractions:
            for degree in range(correlation, 0, -1):
                if degree == correlation:
                    target = contraction.weights_max
                else:
                    idx = correlation - degree - 1
                    target = contraction.weights[idx]

                width = target.shape[1]
                slice_vals = basis_vector[offset : offset + width]
                target.zero_()
                target[0, :width, 0] = torch.tensor(slice_vals, dtype=target.dtype)
                offset += width

    if offset != basis_vector.size:
        raise ValueError('Basis vector length mismatch while scattering weights.')
