{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import torch\n",
    "\n",
    "from mace_jax.haiku.torch import copy_torch_to_jax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prepare data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ase\n",
    "import numpy as np\n",
    "from mace.data.atomic_data import AtomicData\n",
    "from mace.data.utils import config_from_atoms\n",
    "from mace.tools import torch_geometric\n",
    "from mace.tools.torch_geometric.batch import Batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_jax(batch: Batch):\n",
    "    \"\"\"Convert every tensor field in a PyG Batch to JAX arrays.\"\"\"\n",
    "    jax_dict = {}\n",
    "\n",
    "    # .keys is already a list of attribute names\n",
    "    for key in batch.keys:\n",
    "        value = batch[key]\n",
    "\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            arr = value.detach().cpu().numpy()     # to numpy\n",
    "            jax_dict[key] = jnp.asarray(arr)       # to JAX\n",
    "        else:\n",
    "            jax_dict[key] = value                  # leave as-is\n",
    "\n",
    "    return jax_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1007 12:12:09.567000 25044 /home/pbenner/Env/mace-jax/.venv/lib/python3.12/site-packages/torch/utils/cpp_extension.py:118] No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading statistics from `statistics.json`\n"
     ]
    }
   ],
   "source": [
    "def get_statistics(filename = 'statistics.json'):\n",
    "\n",
    "    from mace.tools.multihead_tools import (  # noqa: PLC0415\n",
    "        AtomicNumberTable,\n",
    "    )\n",
    "\n",
    "    print(f\"Reading statistics from `{filename}`\")\n",
    "\n",
    "    with open(filename) as f:\n",
    "        statistics = json.load(f)\n",
    "\n",
    "    statistics['atomic_numbers'] = AtomicNumberTable(statistics['atomic_numbers'])\n",
    "    statistics['atomic_energies'] = [ statistics['atomic_energies'][str(i)] for i in statistics['atomic_numbers'].zs ]\n",
    "\n",
    "    return statistics\n",
    "\n",
    "statistics = get_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_args():\n",
    "\n",
    "    from mace.tools import build_default_arg_parser, check_args  # noqa: PLC0415\n",
    "\n",
    "    arguments = [\n",
    "        \"--name\"              , \"MACE_large_density\",\n",
    "        \"--interaction_first\" , \"RealAgnosticDensityInteractionBlock\",\n",
    "        \"--interaction\"       , \"RealAgnosticDensityResidualInteractionBlock\",\n",
    "        \"--num_channels\"      , \"128\",\n",
    "        \"--max_L\"             , \"2\",\n",
    "        \"--max_ell\"           , \"3\",\n",
    "        \"--num_interactions\"  , \"3\",\n",
    "        \"--correlation\"       , \"3\",\n",
    "        \"--num_radial_basis\"  , \"8\",\n",
    "        \"--MLP_irreps\"        , \"16x0e\",\n",
    "        \"--distance_transform\", \"Agnesi\",\n",
    "        \"--pair_repulsion\",\n",
    "        \"--only_cueq\", \"True\"\n",
    "    ]\n",
    "\n",
    "    args    = build_default_arg_parser().parse_args(arguments)\n",
    "    args, _ = check_args(args)\n",
    "\n",
    "    return args\n",
    "\n",
    "model_args = get_model_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "atoms = ase.Atoms(\n",
    "    symbols = ['H', 'H', 'Ne', 'O'],\n",
    "    positions = np.array(\n",
    "    [\n",
    "        [0.0, 0.0, 0.0],\n",
    "        [0.5, 0.0, 0.0],\n",
    "        [0.0, 0.4, 0.0],\n",
    "        [0.0, 0.3, 0.3],\n",
    "    ]),\n",
    "    cell = np.identity(3),\n",
    "    pbc = [True, True, False],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = config_from_atoms(atoms)\n",
    "config.pbc = [bool(x) for x in config.pbc]\n",
    "x = AtomicData.from_config(config, z_table=statistics['atomic_numbers'], cutoff=2.0)\n",
    "x = torch_geometric.batch.Batch.from_data_list([x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prepare Torch model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mace.tools.model_script_utils import configure_model as configure_model_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_torch(args: argparse.Namespace, statistics):\n",
    "\n",
    "    from mace.data.utils import KeySpecification  # noqa: PLC0415\n",
    "    from mace.tools.multihead_tools import (  # noqa: PLC0415\n",
    "        prepare_default_head,\n",
    "    )\n",
    "\n",
    "    torch.set_default_dtype(torch.float32)\n",
    "\n",
    "    args.mean = statistics['mean']\n",
    "    args.std = statistics['std']\n",
    "    args.compute_energy = True\n",
    "    args.compute_dipole = False\n",
    "    args.key_specification = KeySpecification.from_defaults()\n",
    "    args.heads = prepare_default_head(args)\n",
    "\n",
    "    model, _ = configure_model_torch(\n",
    "        args,\n",
    "        None,\n",
    "        statistics['atomic_energies'],\n",
    "        z_table = statistics['atomic_numbers'],\n",
    "        heads = args.heads,\n",
    "        model_foundation = None,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pbenner/Env/mace-jax/mace-jax/.venv/lib/python3.12/site-packages/cuequivariance_torch/primitives/segmented_polynomial.py:162: UserWarning: cuequivariance_ops_torch is not available. Falling back to naive implementation.\n",
      "  warnings.warn(\n",
      "/home/pbenner/Env/mace-jax/mace-jax/.venv/lib/python3.12/site-packages/cuequivariance_torch/primitives/segmented_polynomial.py:162: UserWarning: cuequivariance_ops_torch is not available. Falling back to naive implementation.\n",
      "  warnings.warn(\n",
      "/home/pbenner/Env/mace-jax/mace-jax/.venv/lib/python3.12/site-packages/cuequivariance_torch/primitives/segmented_polynomial.py:162: UserWarning: cuequivariance_ops_torch is not available. Falling back to naive implementation.\n",
      "  warnings.warn(\n",
      "/home/pbenner/Env/mace-jax/mace-jax/.venv/lib/python3.12/site-packages/cuequivariance_torch/primitives/segmented_polynomial.py:162: UserWarning: cuequivariance_ops_torch is not available. Falling back to naive implementation.\n",
      "  warnings.warn(\n",
      "/home/pbenner/Env/mace-jax/mace-jax/.venv/lib/python3.12/site-packages/cuequivariance_torch/primitives/segmented_polynomial.py:162: UserWarning: cuequivariance_ops_torch is not available. Falling back to naive implementation.\n",
      "  warnings.warn(\n",
      "/home/pbenner/Env/mace-jax/mace-jax/.venv/lib/python3.12/site-packages/cuequivariance_torch/primitives/segmented_polynomial.py:162: UserWarning: cuequivariance_ops_torch is not available. Falling back to naive implementation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_torch = get_model_torch(model_args, statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prepare JAX model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_model_jax(\n",
    "    args,\n",
    "    atomic_energies,\n",
    "    z_table=None,\n",
    "    model_foundation=None,\n",
    "    head_configs=None,\n",
    "):\n",
    "    import ast  # noqa: PLC0415\n",
    "\n",
    "    from e3nn_jax import Irreps  # noqa: PLC0415\n",
    "\n",
    "    from mace_jax import modules  # noqa: PLC0415\n",
    "\n",
    "    model_config = dict(\n",
    "        r_max=args.r_max,\n",
    "        num_bessel=args.num_radial_basis,\n",
    "        num_polynomial_cutoff=args.num_cutoff_basis,\n",
    "        max_ell=args.max_ell,\n",
    "        interaction_cls=modules.interaction_classes[args.interaction],\n",
    "        num_interactions=args.num_interactions,\n",
    "        num_elements=len(z_table),\n",
    "        hidden_irreps=Irreps(args.hidden_irreps),\n",
    "        edge_irreps=Irreps(args.edge_irreps) if args.edge_irreps else None,\n",
    "        atomic_energies=atomic_energies,\n",
    "        apply_cutoff=args.apply_cutoff,\n",
    "        avg_num_neighbors=args.avg_num_neighbors,\n",
    "        atomic_numbers=z_table.zs,\n",
    "        use_reduced_cg=args.use_reduced_cg,\n",
    "        use_so3=args.use_so3,\n",
    "        cueq_config=None,\n",
    "    )\n",
    "    return modules.ScaleShiftMACE(\n",
    "        **model_config,\n",
    "        pair_repulsion=args.pair_repulsion,\n",
    "        distance_transform=args.distance_transform,\n",
    "        correlation=args.correlation,\n",
    "        gate=modules.gate_dict[args.gate],\n",
    "        interaction_cls_first=modules.interaction_classes[args.interaction_first],\n",
    "        MLP_irreps=Irreps(args.MLP_irreps),\n",
    "        atomic_inter_scale=args.std,\n",
    "        atomic_inter_shift=args.mean,\n",
    "        radial_MLP=ast.literal_eval(args.radial_MLP),\n",
    "        radial_type=args.radial_type,\n",
    "        heads=args.heads,\n",
    "        embedding_specs=args.embedding_specs,\n",
    "        use_embedding_readout=args.use_embedding_readout,\n",
    "        use_last_readout_only=args.use_last_readout_only,\n",
    "        use_agnostic_product=args.use_agnostic_product,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_jax(args: argparse.Namespace, statistics):\n",
    "\n",
    "    from mace.data.utils import KeySpecification  # noqa: PLC0415\n",
    "    from mace.tools.multihead_tools import (  # noqa: PLC0415\n",
    "        prepare_default_head,\n",
    "    )\n",
    "\n",
    "    args.mean = statistics['mean']\n",
    "    args.std = statistics['std']\n",
    "    args.compute_energy = True\n",
    "    args.compute_dipole = False\n",
    "    args.key_specification = KeySpecification.from_defaults()\n",
    "    args.heads = prepare_default_head(args)\n",
    "\n",
    "    model = configure_model_jax(\n",
    "        args,\n",
    "        statistics['atomic_energies'],\n",
    "        z_table = statistics['atomic_numbers'],\n",
    "        model_foundation = None,\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2025-10-07 12:13:58,633:jax._src.xla_bridge:864: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "WARNING:jax._src.xla_bridge:An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling JAX model, this may take a while...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pbenner/Env/mace-jax/mace-jax/mace_jax/modules/models.py:71: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in array is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n",
      "  self.atomic_numbers = jnp.array(atomic_numbers, dtype=jnp.int64)\n",
      "/home/pbenner/Env/mace-jax/mace-jax/mace_jax/adapters/e3nn/math/_normalize_activation.py:12: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'>  is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n",
      "  z = jax.random.normal(key, shape=(1_000_000,), dtype=jnp.float64).astype(dtype)\n",
      "/home/pbenner/Env/mace-jax/mace-jax/.venv/lib/python3.12/site-packages/jax/_src/numpy/array_methods.py:124: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in astype is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(self, dtype, copy=copy, device=device)\n",
      "/home/pbenner/Env/mace-jax/mace-jax/.venv/lib/python3.12/site-packages/jax/_src/numpy/array_methods.py:124: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in astype is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(self, dtype, copy=copy, device=device)\n",
      "/home/pbenner/Env/mace-jax/mace-jax/.venv/lib/python3.12/site-packages/cuequivariance_jax/segmented_polynomials/segmented_polynomial_naive.py:248: UserWarning: Explicitly requested dtype float64 requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n",
      "  coefficients = jnp.asarray(d.stacked_coefficients, dtype=math_dtype)\n",
      "/home/pbenner/Env/mace-jax/mace-jax/.venv/lib/python3.12/site-packages/cuequivariance_jax/segmented_polynomials/segmented_polynomial_naive.py:372: UserWarning: Explicitly requested dtype float64 requested in asarray is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n",
      "  jnp.asarray(path.coefficients, dtype=math_dtype),\n"
     ]
    }
   ],
   "source": [
    "def forward_fn(x):\n",
    "    model_jax = get_model_jax(model_args, statistics)\n",
    "    return model_jax(batch_to_jax(x), compute_stress=True)\n",
    "\n",
    "print('Compiling JAX model, this may take a while...')\n",
    "transformed = hk.transform_with_state(forward_fn)\n",
    "rng = jax.random.PRNGKey(42)\n",
    "params, state = transformed.init(rng, x)\n",
    "params = copy_torch_to_jax(model_torch, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "energy: [279.55844]\n",
      "forces: [[-2.0675361e-07 -2.5490959e+02 -9.9468552e+01]\n",
      " [-4.4889748e-07 -4.0847076e+01 -1.9527969e+01]\n",
      " [-7.3667616e-07  9.2627271e+02 -2.2471323e+03]\n",
      " [-1.8160790e-08 -6.3051605e+02  2.3661284e+03]]\n",
      "stress: [[[-3.7244718e+00  4.6954178e-09 -3.1607538e-08]\n",
      "  [ 4.6954178e-09 -1.6263388e+01  1.2610328e+01]\n",
      "  [-3.1607538e-08  1.2610328e+01 -4.7322571e+01]]]\n"
     ]
    }
   ],
   "source": [
    "result_jax, state = transformed.apply(params, state, rng, x)\n",
    "print(f'energy: {result_jax['energy']}\\nforces: {result_jax['forces']}\\nstress: {result_jax['stress']}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "energy: tensor([279.6699], grad_fn=<AddBackward0>)\n",
      "forces: tensor([[ 5.3644e-07, -2.5479e+02, -9.9434e+01],\n",
      "        [ 1.9372e-06, -4.0739e+01, -1.9434e+01],\n",
      "        [-1.2293e-07,  9.2628e+02, -2.2472e+03],\n",
      "        [-1.9372e-07, -6.3075e+02,  2.3660e+03]])\n",
      "stress: tensor([[[-3.7370e+00,  6.8272e-08,  1.9372e-09],\n",
      "         [ 6.8272e-08, -1.6262e+01,  1.2615e+01],\n",
      "         [ 1.9372e-09,  1.2615e+01, -4.7321e+01]]])\n"
     ]
    }
   ],
   "source": [
    "result_torch = model_torch(x, compute_stress=True)\n",
    "print(f'energy: {result_torch['energy']}\\nforces: {result_torch['forces']}\\nstress: {result_torch['stress']}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mace-jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
